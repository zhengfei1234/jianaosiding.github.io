<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>啦e啦啦</title>
  <subtitle>An artist who lives in his fields</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.whilte.cn/"/>
  <updated>2017-04-27T03:51:53.826Z</updated>
  <id>https://blog.whilte.cn/</id>
  
  <author>
    <name>Ewen</name>
    <email>zeliges@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Django 数据库</title>
    <link href="https://blog.whilte.cn/2017/04/26/Django-%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    <id>https://blog.whilte.cn/2017/04/26/Django-数据库/</id>
    <published>2017-04-26T09:32:42.000Z</published>
    <updated>2017-04-27T03:51:53.826Z</updated>
    
    <content type="html"><![CDATA[<p>在Django中的MVC中， 视图用来描述要展现给用户的数据，<br>视图仅决定如何展现数据，而不是展现那些数据。</p>
<ul>
<li>数据库配置</li>
</ul>
<p>关于数据库的配置，这里使用mysql，python3使用mysqlclient库来链接，并更改settings中的设置。</p>
<pre><code>DATABASES = {
    &#39;default&#39;: {
        &#39;ENGINE&#39;: &#39;django.db.backends.mysql&#39;,
        &#39;NAME&#39;: &#39;Django&#39;,
        &#39;USER&#39;: &#39;root&#39;,
        &#39;PASSWORD&#39;: &#39;pwd&#39;,
        &#39;HOST&#39;: &#39;localhost&#39;,
        &#39;PORT&#39;: &#39;3306&#39;,
    }
}
</code></pre><p>在terminal中输入 python3 manage.py shell 来进行测试。</p>
<p>输入如下命令测试数据库的设置：</p>
<pre><code>from django.db import connection
cursor = connection.cursor()
</code></pre><p>如果没有错误信息，那么数据库配置正确。</p>
<ul>
<li>Django App</li>
</ul>
<p>一个project可以包含很多个App及他们的配置。</p>
<p>一个app是一套Django功能的集合，通常包括模型和视图，按Python的包结构的方式存在。</p>
<p>系统对app有一个约定： 如果你使用了Django的数据库层（模型），你 必须创建一个Django app。 模型必须存放在apps中。</p>
<p>创建App</p>
<pre><code>python3 manage.py startapp books
</code></pre><ul>
<li>创建模型</li>
</ul>
<p><em>本例取自DjangoBook的例子</em></p>
<p>我们来假定下面的这些概念、字段和关系：</p>
<p>一个作者有姓，有名及email地址。</p>
<p>出版商有名称，地址，所在城市、省，国家，网站。</p>
<p>书籍有书名和出版日期。 它有一个或多个作者（和作者是多对多的关联关系[many-to-many]）， 只有一个出版商（和出版商是一对多的关联关系[one-to-many]，也被称作外键[foreign key]）</p>
<p>在models.py中输入：</p>
<pre><code>from django.db import models

class Publisher(models.Model):
    name = models.CharField(max_length=30)
    address = models.CharField(max_length=50)
    city = models.CharField(max_length=60)
    state_province = models.CharField(max_length=30)
    country = models.CharField(max_length=50)
    website = models.URLField()

class Author(models.Model):
    first_name = models.CharField(max_length=30)
    last_name = models.CharField(max_length=40)
    email = models.EmailField()

class Book(models.Model):
    title = models.CharField(max_length=100)
    authors = models.ManyToManyField(Author)
    publisher = models.ForeignKey(Publisher)
    publication_date = models.DateField()
</code></pre><p>相当于sql：</p>
<pre><code>CREATE TABLE &quot;books_publisher&quot; (
    &quot;id&quot; serial NOT NULL PRIMARY KEY,
    &quot;name&quot; varchar(30) NOT NULL,
    &quot;address&quot; varchar(50) NOT NULL,
    &quot;city&quot; varchar(60) NOT NULL,
    &quot;state_province&quot; varchar(30) NOT NULL,
    &quot;country&quot; varchar(50) NOT NULL,
    &quot;website&quot; varchar(200) NOT NULL
);
</code></pre><p>最后需要注意的是，我们并没有显式地为这些模型定义任何主键。 除非你单独指明，否则Django会自动为每个模型生成一个自增长的整数主键字段每个Django模型都要求有单独的主键。id</p>
<p>在settings中加入app后，验证模型的有效性：</p>
<pre><code>python3 manage.py makemigrations books
</code></pre><p>运行 makemigrations ，表示告诉Django你已经改变模型，你想储存这些改变。</p>
<p>如果模型没有问题的话，运行：</p>
<pre><code>python3 manage.py sqlmigrate books 001
</code></pre><p>生成sql语句， 命令并没有在数据库中真正创建数据表，只是把SQL语句段打印出来</p>
<p>也可以运行：</p>
<pre><code>python3 manage.py check
</code></pre><p>这将检查项目中的任何问题。不会改变模型。</p>
<pre><code>python3 manage.py migrate
</code></pre><p>这条命令将同步你的模型到数据库，并不能将模型的修改或删除同步到数据库；如果你修改或删除了一个模型，并想把它提交到数据库，migrate并不会做出任何处理。</p>
<ul>
<li>模型内的增删改查</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在Django中的MVC中， 视图用来描述要展现给用户的数据，&lt;br&gt;视图仅决定如何展现数据，而不是展现那些数据。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据库配置&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;关于数据库的配置，这里使用mysql，python3使用mysqlclient库来链接，并更
    
    </summary>
    
    
      <category term="django" scheme="https://blog.whilte.cn/tags/django/"/>
    
  </entry>
  
  <entry>
    <title>Django 模板语言</title>
    <link href="https://blog.whilte.cn/2017/04/08/Django-%E6%A8%A1%E6%9D%BF%E8%AF%AD%E8%A8%80/"/>
    <id>https://blog.whilte.cn/2017/04/08/Django-模板语言/</id>
    <published>2017-04-08T10:18:56.000Z</published>
    <updated>2017-04-24T02:58:47.366Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li>render函数</li>
</ul>
<p>render的参数</p>
<pre><code>render(requests, &#39;index.html&#39;, context)
</code></pre><p>context 也叫上下文， 可以替换index中内容.</p>
<p>比如context为:</p>
<pre><code>context = {
  &#39;title&#39;: &#39;something&#39;,
  &#39;content&#39;: &#39;somehow&#39;
 }
</code></pre><a id="more"></a>
<p>在index中的具体写法，来将context中的内容替换：</p>
<pre><code>{{ title }}
{{ content}}
</code></pre><ul>
<li>static 目录<br>有时侯要使用css， javascript, images等，要设置静态目录文件夹。</li>
</ul>
<p>创建static文件夹，要让Django知道目录的所在。</p>
<p>在setting.py 文件中，需要更新两个变量 STATIC_URL 和 STATICFILES_DIRS 元祖。</p>
<pre><code>
STATIC_URL = &#39;/static&#39;
STATICFILES_DIRS = (os.path.join(BASE_DIR, &#39;static&#39;),)
</code></pre><p>在html文件中，加入 {% load static %} 即可导入static文件。</p>
<ul>
<li>关于数据库</li>
</ul>
<p>上面的context就模拟了， 从数据库取出的数据。从models取出数据，Django其实自带数据库sqlite， 这里我使用mongodb。</p>
<p>要使用mongodb，要现在setting.py 中设置:</p>
<pre><code>from mongoengine import connect
connect(&#39;library&#39;, host=&#39;localhost&#39;, port=27017)
</code></pre><p>在models.py中引入mongoengine，</p>
<pre><code>from django.db import models
from mongoengine import Document, StringField

# 继承 mongoengine的Document类
class LibraryInfo(Document):
# 创建一个字符串Field
    姓名 = StringField(),
    身份证号 = StringField(),
    性别 = StringField(),
    出生日期 = StringField(),
    欠款金额 = StringField(),
    累计借书 = StringField()

# meta 指定集合， 集合要是已经创建的
    meta = {
        &#39;collection&#39;: &#39;math_all&#39;
    }
</code></pre><p>更改以后，要在views.py 中导入：</p>
<pre><code>from tango.models import LibraryInfo
</code></pre><ul>
<li>数据分页</li>
</ul>
<p>数据很多的时候，就需要分页功能。<br>在views.py 中加入</p>
<pre><code>from django.core.paginator import Paginator
# 在函数index中加入
limit = 4
paginator = Paginator(info, limit)
</code></pre><p>Paginator的用法：</p>
<pre><code>from django.core.paginator import Paginator

info = &#39;something and something else something&#39;
# 使用方法，使用方法将 info 分为 4 页。
paginator = Paginator(info, 4)
# 将第一页提出
page1 = paginator.page(1)
# 使用object_list来展现数据
page1.object_list
# 判断一共多少页
page1.has_next()
# 获得page的页数
page1.number
# 获得总页数
page1.paginator.page_number
</code></pre><ul>
<li>模板的继承</li>
</ul>
<p>Django 的模板系统有来创建html页面， 为减少重复和冗余的代码重复， 模板继承可以很优雅的解决。</p>
<p>模板继承就是先创建一个基础模板，然后自定义其中的部分内容。</p>
<p>比如创建一个base.html 内容如下：</p>
<pre><code>&lt;!DOCTYPE HTML PUBLIC &quot;-//W3C//DTD HTML 4.01//EN&quot;&gt;
&lt;html lang=&quot;en&quot;&gt;
&lt;head&gt;
    &lt;title&gt;{% block title %}{% endblock %}

<body>
    <h1>My helpful timestamp site</h1>
    {% block content %}{% endblock %}
    {% block footer %}
    <hr>
    <p>Thanks for visiting my site.</p>
    {% endblock %}
&lt;/body&gt;
&lt;/html&gt;
</body></code></pre><p>其中的 {% block %}  。 所有的  {% block %}  标签告诉模板引擎，子模板可以重载这些部分。 每个 {% block %} 标签所要做的是告诉模板引擎，该模板下的这一块内容将有可能被子模板覆盖。</p>
<p>现在写一个模板来继承它：</p>
<pre><code>{% extends "base.html" %}

{% block title %}The current time{% endblock %}

{% block content %}
<p>It is now {{ current_date }}.</p>
{% endblock %}
</code></pre><p>模板引擎发现了 {% extends %} 标签， 注意到该模板是一个子模板。 模板引擎立即装载其父模板，即本例中的 base.html<br>非常的优雅解决。</p>
<p>以下是使用模板继承的一些点：</p>
<p>1.如果在模板中使用 {% extends %} ，必须保证其为模板中的第一个模板标记。 否则，模板继承将不起作用。</p>
<p>2.一般来说，基础模板中的 {% block %} 标签越多越好。 记住，子模板不必定义父模板中所有的代码块，因此你可以用合理的缺省值对一些代码块进行填充，然后只对子模板所需的代码块进行（重）定义。 俗话说，钩子越多越好</p>
<p>3.如果发觉自己在多个模板之间拷贝代码，你应该考虑将该代码段放置到父模板的某个 {% block %} 中。</p>
<p>4.如果你需要访问父模板中的块的内容，使用 {{ block.super }} 这个标签吧，这一个魔法变量将会表现出父模板中的内容。 如果只想在上级代码块基础上添加内容，而不是全部重载，该变量就显得非常有用了</p>
<p>5.不允许在同一个模板中定义多个同名的 {% block %} 。 存在这样的限制是因为block 标签的工作方式是双向的。 也就是说，block 标签不仅挖了一个要填的坑，也定义了在父模板中这个坑所填充的内容。如果模板中出现了两个相同名称的 {% block %} 标签，父模板将无从得知要使用哪个块的内容。</p>
<p>6.{% extends %} 对所传入模板名称使用的加载方法和 get_template() 相同。 也就是说，会将模板名称被添加到 TEMPLATE_DIRS 设置之后。</p>
<p>7.多数情况下， {% extends %} 的参数应该是字符串，但是如果直到运行时方能确定父模板名，这个参数也可以是个变量。 这使得你能够实现一些很酷的动态功能。</p>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;render函数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;render的参数&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;render(requests, &amp;#39;index.html&amp;#39;, context)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;context 也叫上下文， 可以替换index中内容.&lt;/p&gt;
&lt;p&gt;比如context为:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;context = {
  &amp;#39;title&amp;#39;: &amp;#39;something&amp;#39;,
  &amp;#39;content&amp;#39;: &amp;#39;somehow&amp;#39;
 }
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
    
      <category term="Django" scheme="https://blog.whilte.cn/tags/Django/"/>
    
  </entry>
  
  <entry>
    <title>Django MTV模型</title>
    <link href="https://blog.whilte.cn/2017/04/08/Django-stu/"/>
    <id>https://blog.whilte.cn/2017/04/08/Django-stu/</id>
    <published>2017-04-08T06:18:56.000Z</published>
    <updated>2017-04-24T04:21:18.212Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li>MTV 模型</li>
</ul>
<p>简单的向服务器发送一个requests，服务器会返回一个responses。</p>
<p>具体的操作为：</p>
<p>发送一个请求， requests 会到views区， 找到特定的views， views到models层查找数据， models为托管数据的层级和一些对数据进行操作， 接下来会将找到的数据到templates层，templates来展示数据。这就是简单的 MTV 模型</p>
<p>Django 里更关注的是模型（Model）、模板(Template)和视图（Views），Django 也被称为 MTV 框架 。在 MTV 开发模式中：</p>
<p>M 代表模型（Model），即数据存取层。 该层处理与数据相关的所有事务： 如何存取、如何验证有效性、包含哪些行为以及数据之间的关系等。</p>
<p>T 代表模板(Template)，即表现层。 该层处理与表现相关的决定： 如何在页面或其他类型文档中进行显示。</p>
<p>V 代表视图（View），即业务逻辑层。 该层包含存取模型及调取恰当模板的相关逻辑。 你可以把它看作模型与模板之间的桥梁。</p>
<ul>
<li>创建 Django 项目</li>
</ul>
<p>使用pycharm中的terminal：</p>
<pre><code>python3 manage.py startproject tango
</code></pre><ul>
<li><strong>init</strong>.py:这是一个空的脚本,用来告诉Python编译器这个目录是一个Python包.</li>
<li>settings.py:用来存储Django项目设置的文件.</li>
<li>urls.py:用来存储项目里的URL模式.</li>
<li>wsgi.py:用来帮助你运行开发服务,同时可以帮助部署你的生产环境.</li>
</ul>
<p>在新的 Djangoapp 项目中的文件：</p>
<ul>
<li><strong>init</strong>.py，前面的功能一样.</li>
<li>models.py,一个存储你的应用中数据模型的地方 - 在这里描述数据的实体和关系.</li>
<li>tests.py,存储你应用的测试代码.</li>
<li>views.py,在这里处理用户请求和响应.</li>
<li>admin.py,在这里你可以向Django注册你的模型,它会为你创建Django的管理界面.</li>
</ul>
<p>新建一个项目后要在 setting.py 中设置 INSTALLED_APPS 元组 中 加入新建app的名字， 这样django就可以识别le。</p>
<ul>
<li>创建视图</li>
</ul>
<p>在views.py 中， 加入：</p>
<pre><code>from django.shortcuts import render
def index(requests):
    return render(requests, &#39;index.html&#39;)
</code></pre><p>需要在templates中新建一个html文件为index。</p>
<ul>
<li>分配url<br>在urls.py中，添加代码：</li>
</ul>
<pre><code>from django.conf.urls import url
from django.contrib import admin
from tango.views import index # 导入views层中的index

urlpatterns = [
    url(r&#39;^admin/&#39;, admin.site.urls),
    url(r&#39;^index/&#39;, index) # 增加urls
]
</code></pre><ul>
<li>运行服务</li>
</ul>
<p>在termainl中：</p>
<pre><code>python3 manage.py runserver
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;MTV 模型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简单的向服务器发送一个requests，服务器会返回一个responses。&lt;/p&gt;
&lt;p&gt;具体的操作为：&lt;/p&gt;
&lt;p&gt;发送一个请求， requests 会到views区， 找到特定的views， views到mode
    
    </summary>
    
    
      <category term="Django" scheme="https://blog.whilte.cn/tags/Django/"/>
    
  </entry>
  
  <entry>
    <title>selenium实现浏览器下拉页面</title>
    <link href="https://blog.whilte.cn/2017/04/04/selenium%E6%9D%A5%E6%A8%A1%E4%BB%BF%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%8B%E6%8B%89%E9%A1%B5%E9%9D%A2/"/>
    <id>https://blog.whilte.cn/2017/04/04/selenium来模仿浏览器下拉页面/</id>
    <published>2017-04-04T09:55:35.000Z</published>
    <updated>2017-04-05T08:38:51.922Z</updated>
    
    <content type="html"><![CDATA[<p>在我爬取美团数据的时候，发现美团的整个页面数据采用ajax来加载，ajax 就是异步的javascript和XML, 我也不是很懂， 近期打算学习js，它的功能就是网站不需要使用单独的页面请求就可以和服务器进行交互，表单采用Ajax与服务器通信。</p>
<p>要想获得全部的数据，需要下拉到底页面才会完全加载本页面的全部内容。</p>
<p>这里其时有两种办法，来进行爬取：<br><a id="more"></a></p>
<ol>
<li><p>抓包分析请求，找到真实的请求，来模拟请求，获得数据。</p>
</li>
<li><p>使用selenium来进行爬取，但是其效率不高。</p>
</li>
</ol>
<p>由于我爬取的数据量较小，我只需要几个分类下的数据。我采用第二种。（第一种，模拟起来很麻烦）。</p>
<pre><code># 导入第三方库
from selenium import webdriver
import time
from bs4 import BeautifulSoup

# 你可以使用 phantomjs 或者使用 chrome
driver = webdriver.PhantomJS(executable_path=&#39;/home/jasd/python/phantomjs/bin/phantomjs&#39;)
# driver = webdriver.Chrome()

def get_info(page):

    driver.get(full_url)
    time.sleep(2)
# 这里我使用 js 的方法，来实现下拉页面
    js = &quot;window.scrollTo(0,document.body.scrollHeight)&quot;
# 使用的执行 JavaScript
    driver.execute_script(js)
# 休眠 3 秒种
    time.sleep(3)
# 重复执行多次， 确保页面已完全加载
    js = &quot;window.scrollTo(0,document.body.scrollHeight)&quot;
    driver.execute_script(js)
    time.sleep(3)
    js = &quot;window.scrollTo(0,document.body.scrollHeight)&quot;
    driver.execute_script(js)
    time.sleep(3)
    js = &quot;window.scrollTo(0,document.body.scrollHeight)&quot;
    driver.execute_script(js)
    time.sleep(3)
    js = &quot;window.scrollTo(0,document.body.scrollHeight)&quot;
    driver.execute_script(js)
    time.sleep(3)
    js = &quot;window.scrollTo(0,document.body.scrollHeight)&quot;
    driver.execute_script(js)
    time.sleep(3)
    js = &quot;window.scrollTo(0,document.body.scrollHeight)&quot;
    driver.execute_script(js)
    time.sleep(3)

# 这里使用 page_source 的方法， 我还是比较喜欢使用BeautifulSoup， 虽然效率有点低
    pageSoure = driver.page_source
    soup = BeautifulSoup(pageSoure, &#39;lxml&#39;)
# 退出
    driver.quit()

get_info(i)
</code></pre><p>我觉得代码写的很挫。。。。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在我爬取美团数据的时候，发现美团的整个页面数据采用ajax来加载，ajax 就是异步的javascript和XML, 我也不是很懂， 近期打算学习js，它的功能就是网站不需要使用单独的页面请求就可以和服务器进行交互，表单采用Ajax与服务器通信。&lt;/p&gt;
&lt;p&gt;要想获得全部的数据，需要下拉到底页面才会完全加载本页面的全部内容。&lt;/p&gt;
&lt;p&gt;这里其时有两种办法，来进行爬取：&lt;br&gt;
    
    </summary>
    
    
      <category term="spider" scheme="https://blog.whilte.cn/tags/spider/"/>
    
  </entry>
  
  <entry>
    <title>简单推荐(python)</title>
    <link href="https://blog.whilte.cn/2017/04/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F-python/"/>
    <id>https://blog.whilte.cn/2017/04/01/推荐系统-python/</id>
    <published>2017-04-01T08:08:46.000Z</published>
    <updated>2017-04-05T08:38:29.930Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li>协作型过滤</li>
</ul>
<p>一个协作型过滤算法通常会对一大群人进行搜索， 并从中找出与我们品味相近的一小群人。算法会对这些人所偏爱的内容进行分析，并将它们组合构建一个经过排名的列表。</p>
<ul>
<li>寻找相近的用户</li>
</ul>
<p>有很多相似度评价值的体系，我将写的是 欧几里德距离和皮尔逊相关系数。</p>
<ul>
<li>欧几里德距离</li>
</ul>
<p>欧几里得距离就是，比如在平面直角坐标系中，欧几里得距离就是两点的距离方程。</p>
<p>两点距离公式：</p>
<p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bfa1815838113388d78c9402bba7308d734a4af2" alt="oujilide"></p>
<a id="more"></a>
<p>欧几里得距离计算的代码</p>
<pre><code>from math import sqrt


def sim_distance(dataset, person1, person2):
# common 为两者共同的之处
        common = {}
        for item in dataset[person1]:
                if item in dataset[person2]:
                        common[item] = 1

# 没有共同之处， 则返回0
        if len(common) == 0: return 0

# 计算差值的平方和
        sum_of_squares = sum([pow(dataset[person1][item] - dataset[person2][item], 2) for item in dataset[person1] if item in dataset[person2]])

# 我将函数值加1， 避免除数为0的情况， 并取倒数
        return 1 / (1 + sqrt(sum_of_squares))
</code></pre><ul>
<li>皮尔逊相关系数</li>
</ul>
<p>关于皮尔逊相关系数，首先是协方差。</p>
<p>协方差是描述相互关联程度的一个特征数就是协方差。定义：</p>
<blockquote>
<p>设（X, Y) 是二维的随机变量， 若 E[(X - E(X))(Y - E(Y))] 存在，则称此数学期望为 X 和 Y 的协方差， 或称为 X与Y的相关（中心）距。</p>
</blockquote>
<p>协方差为 X的偏差与Y的偏差的乘积的数学期望。</p>
<p>协方差公式：</p>
<p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3b2f5356a89c413a67fbb16292ed80e3187fbc86" alt="cov"></p>
<p>协方差是有量纲的量，比如X表示人的身高， 单位是 m， Y表示人的体重， 单位是 kg，则 Cov(X, Y)带有量纲（m.kg), 为了消除量纲的影响，对协方差除以相同量纲的量，就得到相关系数。</p>
<p>相关系数公式：</p>
<p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8b0d0608b5f85d24a9c572f8d1b5769289664dfb" alt="pearson"></p>
<p>除以X和Y的标准差的乘积。</p>
<p>公式1:</p>
<p><img src="http://oava7ylm5.bkt.clouddn.com/19961_12788382715I69.gif" alt="foum1"></p>
<p>公式2:</p>
<p><img src="http://oava7ylm5.bkt.clouddn.com/19961_1278843346L5kz.gif" alt="foum2"></p>
<p>公式2 为样本的相关系数</p>
<p>皮尔逊相关系数计算代码：</p>
<pre><code>from math import sqrt


def sim_pearson(prefs, p1, p2):
    common = {}
    for item in prefs[p1]:
        if item in prefs[p2]:
            common[item] = 1

    n = len(common)
    if n == 0: return 0

    sum1 = sum([prefs[p1][it] for it in common])
    sum2 = sum([prefs[p2][it] for it in common])

    sum1Sq = sum([pow(prefs[p1][it], 2) for it in common])
    sum2Sq = sum([pow(prefs[p2][it], 2) for it in common])

    pSum = sum([prefs][p1][it]*prefs[p2][it] for it in common)

    # 计算
    xfc = pSum - (sum1 * sum2) / n
    bzc = sqrt((sum1Sq - pow(sum1, 2) / n) * (sum2Sq - pow(sum2, 2) / n))

    if bzc == 0 : return 0
    pearson = xfc / bzc

    return pearson
</code></pre><ul>
<li>推荐物品</li>
</ul>
<p>一个受更多人评论的物品会对结果产生更大的影响， 为了修正这样的问题，要除以它所代表的对着个物品评论的人的相似度之和。</p>
<pre><code># 利用所有人的评价值的加权平均， 为某人提供建议
def getRecommendations(prefs, person, similarity=sim_pearson):
    totals = {}
    simSums = {}
    for other in prefs:
    # 不和自己作比较
        if other == person:
            continue
        sim = similarity(prefs, person, other)

        # 忽略评价值小于0或等于0的情况
        if sim &lt; 0:
            continue

        for item in prefs[other]:

        # 只对自己还未评价的物品评价
            if item not in prefs[person] or prefs[person][item] == 0:

                totals.setdefalult(item, 0)
                totals[item] += prefs[other][item]*sim

                simSums.setdefalult(item, 0)
                simSums[item] += sim

    # 生成列表
    rankings = [(total / simSums[item], item) for item in totals.items()]

    rankings.sort()
    rankings.reverse()
    return rankings
</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;协作型过滤&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个协作型过滤算法通常会对一大群人进行搜索， 并从中找出与我们品味相近的一小群人。算法会对这些人所偏爱的内容进行分析，并将它们组合构建一个经过排名的列表。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;寻找相近的用户&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有很多相似度评价值的体系，我将写的是 欧几里德距离和皮尔逊相关系数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;欧几里德距离&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;欧几里得距离就是，比如在平面直角坐标系中，欧几里得距离就是两点的距离方程。&lt;/p&gt;
&lt;p&gt;两点距离公式：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/bfa1815838113388d78c9402bba7308d734a4af2&quot; alt=&quot;oujilide&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://blog.whilte.cn/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>赶集网爬虫</title>
    <link href="https://blog.whilte.cn/2017/03/28/%E8%B5%B6%E9%9B%86%E7%BD%91%E7%88%AC%E8%99%AB/"/>
    <id>https://blog.whilte.cn/2017/03/28/赶集网爬虫/</id>
    <published>2017-03-28T08:08:46.000Z</published>
    <updated>2017-04-25T05:25:37.201Z</updated>
    
    <content type="html"><![CDATA[<p>爬虫地址：<a href="https://github.com/jianaosiding/crawl1" target="_blank" rel="external">spider</a></p>
<p>赶集网的爬虫，爬取赶集网上的招聘的板块上的信息并储存。<br><em>算是以深度优先来进行爬取</em></p>
<p><em>爬虫的目的是收集数据，数据的格式要便于处理和分析。</em></p>
<h4 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h4><ul>
<li>MongoDB</li>
<li>requests</li>
<li>pymongo</li>
<li>BeautifulSoup and lxml</li>
</ul>
<h4 id="关于爬虫"><a href="#关于爬虫" class="headerlink" title="关于爬虫"></a>关于爬虫</h4><h5 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h5><ol>
<li>确定已安装MongoDB，Python3.5, 浏览器建议使用Chrome。</li>
<li><p>安装需要的Python的第三方库:</p>
<pre><code> 在 terminal(cmd) 中输入：

 win:

 pip install requests, BeautifulSoup4, pymongo, lxml

 linux:

 sudo pip3 install requests, BeautifulSoup4, pymongo, lxml
</code></pre></li>
<li><p>如果你知道更好(不知道也可以)：</p>
<ul>
<li>一点 HTML CSS HTTP and MongoDB</li>
</ul>
</li>
</ol>
<h5 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h5><p><img src="http://oava7ylm5.bkt.clouddn.com/test.jpg" alt="test"></p>
<ul>
<li><p>Spider 1 从<a href="http://bj.ganji.com/zhaopin/" target="_blank" rel="external">列表页</a>中获取页面下全部的个分类的链接，然后将分类的链接储存到 url_list 这个collection中。</p>
</li>
<li><p>Spider 2 从url_list 中获取储存的链接并解析，获取链接的详情页，将详情页中的想要抓取的信息储存到 item_info 中。</p>
</li>
</ul>
<h5 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h5><p><img src="http://oava7ylm5.bkt.clouddn.com/liuchengtu.png" alt="liuchengtu"></p>
<h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><h5 id="首先要观察爬取网站的页面结构"><a href="#首先要观察爬取网站的页面结构" class="headerlink" title="首先要观察爬取网站的页面结构"></a>首先要观察爬取网站的页面结构</h5><ul>
<li>首先从<a href="http://bj.ganji.com/zhaopin/" target="_blank" rel="external">起始页</a>来看，我的目的是将其中的 销售， 行政， 后勤等这样的类别的链接提取出来。</li>
<li>其次到这样的<a href="http://bj.ganji.com/zpshichangyingxiao/" target="_blank" rel="external">类别页</a>中， 我的目的是将 所有招聘职位的链接 爬取下来并储存到url_list表中，</li>
<li>然后从数据库url_list表中，迭代其中的链接，解析出<a href="http://bj.ganji.com/zpfangjingjiren/2593320747x.htm" target="_blank" rel="external">详情页</a>，提取出我想要的信息。就是这样一步一步的深入它的页面结构</li>
</ul>
<h5 id="channel-url文件-为将招聘板块中的招聘职位分类的链接提取出来。"><a href="#channel-url文件-为将招聘板块中的招聘职位分类的链接提取出来。" class="headerlink" title="channel_url文件 为将招聘板块中的招聘职位分类的链接提取出来。"></a>channel_url文件 为将招聘板块中的招聘职位分类的链接提取出来。</h5><p>这里的 <a href="http://docs.python-requests.org/zh_CN/latest/index.html" target="_blank" rel="external">requests</a> 的作用:</p>
<blockquote>
<p>Requests 唯一的一个非转基因的 Python HTTP 库，人类可以安全享用。<br><em>警告：非专业使用其他 HTTP 库会导致危险的副作用，包括：安全缺陷症、冗余代码症、重新发明轮子症、啃文档症、抑郁、头疼、甚至死亡。</em></p>
</blockquote>
<p>requests 的基本用法:</p>
<pre><code>import requests
# 发起get请求
data = requests.get(&#39;http://www.baidu.com&#39;)
# 打印data
print(data.text)
# .....
# 其中还可以设置 headers and proxies
# headers 是访问一个网站的头部信息， 通常在可在 浏览器的开发者工具中看到
# 如 chrome: 检查 &gt; network &gt; headers
# 使用headers 的作用是看起来像一个人一样的访问
# 由于网站可能限制了一个ip访问的频率，所以使用代理ip
</code></pre><p><em>关于代理ip</em> 可以在 <a href="http://cn-proxy.com/" target="_blank" rel="external">http://cn-proxy.com/</a> 找到， 当然也可以写一个爬虫，将代理爬下来。</p>
<p>有关http的头字段可以参见 <a href="https://zh.wikipedia.org/wiki/HTTP%E5%A4%B4%E5%AD%97%E6%AE%B5%E5%88%97%E8%A1%A8" target="_blank" rel="external">http头字段</a></p>
<p><em>headers 的位置</em>：<br><img src="http://oava7ylm5.bkt.clouddn.com/headers.png" alt="img"></p>
<p>这里的 BeautifulSoup and lxml的作用：</p>
<blockquote>
<p>Beautiful Soup 是一个可以从HTML或XML文件中提取数据的Python库, 这里我使用 lxml 来解析。</p>
</blockquote>
<p><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#" target="_blank" rel="external">BeautifulSoup</a> 的基本用法：</p>
<pre><code>from bs4 import BeautifulSoup
# 以下是一个非常经典的写法，其中 data.text是做汤的原料， lxml 是菜谱，BeautifulSoup 将其做成
soup = BeautifulSoup(data.text, &#39;lxml&#39;)
# BeautifulSoup 有 select and find 两种提取信息的方式
# select 用的是css选择器
img = soup.select(&#39;#lg &gt; img&#39;)
# find 方法可以根据标签和属性来提取信息
img = soup.find(&#39;img&#39;, {&#39;hidefocus&#39;: &quot;true&quot;})
# 此外 find 还有 findAll() and find_all() 方法 可参见文档
</code></pre><p>channel_url 中定义了一个函数， 来进行获取链接，我这里将运行的结果，赋给了channel_list, 当然也可以放进数据库。<br><img src="http://oava7ylm5.bkt.clouddn.com/chanenl_list.png" alt="images"></p>
<h5 id="page-spider-是进行爬取的主要文件，-其中定义两个函数，是进行爬取信息的两个爬虫。"><a href="#page-spider-是进行爬取的主要文件，-其中定义两个函数，是进行爬取信息的两个爬虫。" class="headerlink" title="page_spider 是进行爬取的主要文件， 其中定义两个函数，是进行爬取信息的两个爬虫。"></a>page_spider 是进行爬取的主要文件， 其中定义两个函数，是进行爬取信息的两个爬虫。</h5><p>这里的 pymongo:</p>
<blockquote>
<p>PyMongo is a Python distribution containing tools for working with MongoDB, and is the recommended way to work with MongoDB from Python. This documentation attempts to explain everything you need to know to use PyMongo.</p>
</blockquote>
<p>pymongo 的基本用法：</p>
<pre><code>import pymongo
# 连接pymongo
client = pymongo.MongoClient(&#39;localhost&#39;, 27017)
# 连接到数据库
ganji = client[&#39;ganji&#39;]
# 连接到数据库中的一个表(集合)
url_list = ganji[&#39;url_list&#39;]
# pymongo 的一些方法: 单个插入数据
url_list1.insert_one({&#39;url&#39;: url})
# 对数据库中的数据进行计数
url_list1.find().count()
</code></pre><p>关于 <em>正则表达式</em>：<br>python 中的正则表达式为 re模块， 基本用法：</p>
<pre><code>import re
# 进行正则匹配
re.complie(&#39;^(https?://)?bj\.ganji\.com.*\.htm&#39;)
</code></pre><p>正则表达式可以参见：<a href="http://www.runoob.com/python/python-reg-expressions.html" target="_blank" rel="external">正则表达式</a></p>
<p>一个正则表达式的测试网站：<a href="http://www.pyregex.com/" target="_blank" rel="external">PyRegex</a></p>
<h6 id="其中的函数-get-link-form-的一些细节："><a href="#其中的函数-get-link-form-的一些细节：" class="headerlink" title="其中的函数 get_link_form 的一些细节："></a>其中的函数 get_link_form 的一些细节：</h6><ol>
<li><p>url_views 是对其url 的观察构造的，例如 <a href="http://bj.ganji.com/zpshichangyingxiao/o3/" target="_blank" rel="external">http://bj.ganji.com/zpshichangyingxiao/o3/</a> 为其第三页的url， 我将其写成 url_views = ‘{0}o{1}/‘.format(channel, str(page)) ， channel 和 page 为这个函数的参数</p>
</li>
<li><p>我的计划是爬去100页， 但是有的页面可能没有100页， 用if来判断是否有要爬去的信息，有则爬，无则pass， 具体的判断的依据是 <em>每个页面下都会有的 下一页的选项</em>。</p>
</li>
<li><p>由于findAll所返回的对象是一个列表，所以用for循环来进行迭代并储存。</p>
</li>
</ol>
<h6 id="其中get-item-info-的细节："><a href="#其中get-item-info-的细节：" class="headerlink" title="其中get_item_info 的细节："></a>其中get_item_info 的细节：</h6><ol>
<li>与上一个函数相似，这个主要是将每一个职位招聘的详情信息，提取并储存。但是要判断一下，这个商品是否已经成交或者下架，如果成交或者下架，那么访问这个页面时状态码应该返回 404. 所以用if判断状态码来进行爬去。</li>
</ol>
<h6 id="main-文件主要是调用page-spider的爬虫进行爬取"><a href="#main-文件主要是调用page-spider的爬虫进行爬取" class="headerlink" title="main 文件主要是调用page_spider的爬虫进行爬取"></a>main 文件主要是调用page_spider的爬虫进行爬取</h6><p>multiprocessing 为python的多进程模块，采用多进程爬取。</p>
<pre><code>from multiprocessing import Pool
# 如果要启动大量的子进程，可以用进程池的方式批量创建子进程
pool = Pool(4)
for link in url_list1.find({}, {&#39;_id&#39;: 0, &#39;url&#39;: 1}):
        pool.apply(get_item_info, args=(link[&#39;url&#39;],))
pool.close()
pool.join()
# pool = Pool(4) 就可以运行 4 个进程， Pool的默认大小是CPU的核数
# 对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了
# map 函数是一个语法糖， map 函数接受两个参数， 一个序列， 一个函数， 并把结果通过Iterator(迭代器)返回
test = map(lambda x: x*x, [1, 2, 3, 4, 5, 6, 7, 8, 9])
test 为 &lt;map object at 0x7f998e02c7b8&gt;
list(test)
[1, 4, 9, 16, 16, 25, 36, 49, 64, 81]
</code></pre><h5 id="count-文件主要是在main-py-运行时，-每5秒来检查一次数据库中数据的数量。"><a href="#count-文件主要是在main-py-运行时，-每5秒来检查一次数据库中数据的数量。" class="headerlink" title="count 文件主要是在main.py 运行时， 每5秒来检查一次数据库中数据的数量。"></a>count 文件主要是在main.py 运行时， 每5秒来检查一次数据库中数据的数量。</h5><h4 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h4><p><img src="http://oava7ylm5.bkt.clouddn.com/xiaoguotu1.png" alt="xiaoguotu1"><br><img src="http://oava7ylm5.bkt.clouddn.com/xiaoguotu2.png" alt="xiaoguotu2"></p>
<h4 id="爬取结果"><a href="#爬取结果" class="headerlink" title="爬取结果"></a>爬取结果</h4><p>我的MongoDB中， 其中 url_list的 count 为:<em>120166</em>, item_info的 count为:<em>30726</em>.</p>
<p>可能是我url_list先于item_info爬取，url_list有些链接已经失效。</p>
<h4 id="体会"><a href="#体会" class="headerlink" title="体会"></a>体会</h4><p>我之前写过58同城的爬虫和这个类似， 但是写这个的时候bug明显更多。</p>
<p>不同的网站会有不同的抓取策略。</p>
<p><em>写爬虫会有很多的坑，验证码，采集JavaScript，模拟登录等都是要克服的。</em></p>
<p>还有一些网站的数据加载方式， 比如美团，美团的数据加载使用的是 ajax(异步加载JavaScript和XML)，用selenium虽然可以,但是效率低。抓包分析效率高，但是很难模仿请求。我比较偏于分析请求。所以要了解网站的一些常见的反爬措施和网页的结构。</p>
<p>赶集网这种网站页面结构很适合爬取， 但是它对同一ip的访问频率会有限制。</p>
<p>这个程序的效率还可以优化，<em>例如 使用 lxml 来解析网页， 速度会快大概10倍的样子， 还可以利用 异步非阻塞 优化， </em>虽然我不会。 我会继续学习的。**</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;爬虫地址：&lt;a href=&quot;https://github.com/jianaosiding/crawl1&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;spider&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;赶集网的爬虫，爬取赶集网上的招聘的板块上的信息并储存。&lt;br&gt;&lt;em
    
    </summary>
    
    
      <category term="python" scheme="https://blog.whilte.cn/tags/python/"/>
    
      <category term="spider" scheme="https://blog.whilte.cn/tags/spider/"/>
    
  </entry>
  
  <entry>
    <title>python的一些糖</title>
    <link href="https://blog.whilte.cn/2017/03/16/python-tip/"/>
    <id>https://blog.whilte.cn/2017/03/16/python-tip/</id>
    <published>2017-03-16T10:09:01.000Z</published>
    <updated>2017-04-05T08:38:45.062Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li>reduce</li>
</ul>
<p>reduce 把一个函数作用在序列上， 这个函数必须有两个参数， 它把结果继续作用在序列的下一个元素上。</p>
<p>比如一个列表内元素的所用的乘积：</p>
<pre><code>from functools import reduce

list = [1, 2, 3, 4, 5, 6, 7, 8, 9]

reduce(lambda x, y: x*y, list)

return 362880
</code></pre><a id="more"></a>
<ul>
<li>map</li>
</ul>
<p>map 函数接受两个参数， 一个序列， 一个函数， 并把结果通过Iterator(迭代器)返回</p>
<pre><code>test = map(lambda x: x*x, [1, 2, 3, 4, 5, 6, 7, 8, 9])

test 为 &lt;map object at 0x7f998e02c7b8&gt;

list(test)

[1, 4, 9, 16, 16, 25, 36, 49, 64, 81]
</code></pre><ul>
<li>filter</li>
</ul>
<p>filter 函数接受两个参数， 一个函数， 一个序列， 并把函数作用与序列的每一个元素， 并通过True or False， 来判断是否保留元素。</p>
<pre><code>test = filter(lambda x : x &gt; 5, [1, 3, 4, 6, 7, 8, 9])

list(test)

[6, 7, 8, 9]
</code></pre><ul>
<li>int</li>
</ul>
<p>int 函数可以返回 整数</p>
<pre><code>int(12.3)
return 12
</code></pre><p>而且 int 函数可以返回， 字符串的进制数</p>
<pre><code>int(&#39;0001&#39;, 2)

return 1

其中的 ‘0001’ 必须以字符串的形式输入， 返回的是2进制数
</code></pre><ul>
<li>join</li>
</ul>
<p>join 函数 string.join(seq), 其中seq是一个序列， string为字符串，</p>
<pre><code>&#39;+&#39;.join(map(str,range(7)))

return &#39;0+1+2+3+4+5+6&#39;
</code></pre><ul>
<li>operator 模块</li>
</ul>
<p>python 内置的模块，operator为c代码编写， 其速度更快。</p>
<p>算术操作符：</p>
<pre><code>import operator

operator.add(1, 2)
operator.sub(1, 2)
operator.mul(1, 2)
operator.turediv(1, 2)# 浮点除法
operator.floortdiv(1, 2)# 整数除法
operator.mod(1, 2)
</code></pre><p>其他可参见 (operator)[<a href="https://docs.python.org/3/library/operator.html" target="_blank" rel="external">https://docs.python.org/3/library/operator.html</a>]</p>
<ul>
<li>list and string</li>
</ul>
<p>list 的一个很实用的方法 list.count(obj),</p>
<p>string.count(obj)</p>
<pre><code>test = &#39;fhaognaorghksangjalgjl&#39;

test.count(&#39;n&#39;)# return 2

list(test).count(&#39;n&#39;)# return 2
</code></pre><ul>
<li>math</li>
</ul>
<p>math 函数， math.modf() 返回数字的小数部分，<br>其中 is_integer() 函数</p>
<pre><code>test = math.sqrt(155)

&gt;&gt;&gt; math.modf(test)
(0.4498995979887326, 12.0)

&gt;&gt;&gt; (2.0).is_integer()
True
&gt;&gt;&gt; (3.2).is_integer()
False
</code></pre><ul>
<li>@classmethod and @staticmethod 的区别</li>
</ul>
<p>py中要想调用类中的一个方法， 要先实例化一个对象在调用。<br>而使用@classmethod和@staticmethod则不需要实例化， 直接使用 类名.方法名来调用</p>
<p>很优雅，<br>他们的区别：</p>
<p>@staticmethod 不需要表达自身的对象的self参数和自身类的cls的参数</p>
<p>@classmethod 不需要表达自身de对象的self参宿， 但是第一个参数必须是表达自身类的cls参数</p>
<pre><code>class A(object):  
    bar = 1  
    def foo(self):  
        print &#39;foo&#39;  

    @staticmethod  
    def static_foo():  
        print &#39;static_foo&#39;  
        print A.bar  

    @classmethod  
    def class_foo(cls):  
        print &#39;class_foo&#39;  
        print cls.bar  
        cls().foo()  

A.static_foo()  
A.class_foo()
</code></pre><p>输出</p>
<pre><code>static_foo
1
class_foo
1
foo
</code></pre><ul>
<li>itertools</li>
</ul>
<p>itertools用于高效循环的迭代函数集合</p>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;reduce&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;reduce 把一个函数作用在序列上， 这个函数必须有两个参数， 它把结果继续作用在序列的下一个元素上。&lt;/p&gt;
&lt;p&gt;比如一个列表内元素的所用的乘积：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from functools import reduce

list = [1, 2, 3, 4, 5, 6, 7, 8, 9]

reduce(lambda x, y: x*y, list)

return 362880
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://blog.whilte.cn/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>关于学校图书馆网站</title>
    <link href="https://blog.whilte.cn/2017/01/27/%E5%85%B3%E4%BA%8E%E5%AD%A6%E6%A0%A1%E5%9B%BE%E4%B9%A6%E9%A6%86%E7%BD%91%E7%AB%99/"/>
    <id>https://blog.whilte.cn/2017/01/27/关于学校图书馆网站/</id>
    <published>2017-01-27T02:57:24.000Z</published>
    <updated>2017-04-26T09:33:17.633Z</updated>
    
    <content type="html"><![CDATA[<p>前段时间看完了python网络数据收集这本书，自己想练习一下。<br>联想到刚入学时的时候登陆学校图书馆网站，初始密码是学号，觉得很鸡肋，<br>觉得用模拟登陆和验证码识别，就可以爬下我们学院每个人的图书借阅记录和个人信息，顺便写一个自动借阅的脚本。<br>顿时很有兴趣，就动手了。</p>
<ul>
<li>验证码</li>
</ul>
<p>验证码很顺利，学校的图书馆的验证码很容易识别，就用pytesseract很容易。</p>
<pre><code>
import pytesseract
from PIL import Image
import requests
captcha_url =&quot;http://202.196.13.8:8080/reader/captcha.php&quot;
img = requests.get(captcha_url).content
code = pytesseract.image_to_string(img)
print(code)
</code></pre><ul>
<li>cookie</li>
</ul>
<p>其实cookie的处理，我也学习一些库的使用，起初实验了很多次，都是无法进入登陆后的页面，<br>后来用了一个库http.cookiejar就顺利解决了。</p>
<pre><code># 打开session。
session = requests.Session()
# 保存cookies
session.cookies = cookielib.LWPCookieJar(filename=&#39;cookies&#39;)
try:
    # session加载cookies
    session.cookies.load(ignore_discard=True)
except:
    print(&quot;Cookie 未能加载&quot;)
</code></pre><ul>
<li>编码</li>
</ul>
<p>编码真是蛋疼到爆呀，每次print都是一堆乱码，编码让我怀疑人生的说，<br>我也知道cmd上print不了UTF-8，我的atom也是如此，pycharm也要改一下encoding，<br>这是才知python的idle才是最好用的。</p>
<pre><code>info = session.get(&quot;http://202.196.13.8:8080/reader/redr_info.php&quot;,
                   headers=headers).content.decode(&quot;UTF-8&quot;)
</code></pre><h4 id="code"><a href="#code" class="headerlink" title="code:"></a>code:</h4><pre><code>
import requests
from bs4 import BeautifulSoup
import time
import http.cookiejar as cookielib
import pymongo
import pytesseract
from PIL import Image


# 连接mongodb
client = pymongo.MongoClient(&quot;localhost&quot;, 27017, connect=False)
library = client[&#39;library&#39;]
library_info = library[&#39;ys13&#39;]
# 设置headers
headers = {
    &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&#39;,
    &#39;Accept-Encoding&#39;: &#39;gzip, deflate, sdch&#39;,
    &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8&#39;,
    &#39;Cache-Control&#39;: &#39;max-age=0&#39;,
    &#39;Connection&#39;: &#39;keep-alive&#39;,
    &#39;Host&#39;: &#39;202.1X6.13.X:8080&#39;,
    &#39;Upgrade-Insecure-Requests&#39;: &#39;1&#39;,
    &#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36&#39;
}
# 打开session。
session = requests.Session()
# 保存cookies
session.cookies = cookielib.LWPCookieJar(filename=&#39;cookies&#39;)
try:
    # session加载cookies
    session.cookies.load(ignore_discard=True)
except:
    print(&quot;Cookie 未能加载&quot;)



def get_captcha():
    captcha_url = &quot;http://202.196.13.8:8080/reader/captcha.php&quot;
    x = 0
    bin = session.get(captcha_url, headers=headers).content
    with open(&quot;/home/jasd/python/%s.jpg&quot; % x, &quot;wb&quot;)as file:
        file.write(bin)
    image = Image.open(&quot;/home/jasd/python/0.jpg&quot;)
    code = pytesseract.image_to_string(image)
    return code


# get_chaptcha()主要使用tesseract来通过subprocess来将验证码识别
# login()为登陆函数，可采集集信息， 并放置于mongodb中


def login(xuehao):
    from_data = {
        &#39;number&#39;: xuehao,
        &#39;passwd&#39;: xuehao,
        &#39;captcha&#39;: get_captcha(),
        &#39;select&#39;: &#39;cert_no&#39;,
        &#39;returnUrl&#39;: &#39;&#39;,
        }
    session.post(&quot;http://202.196.13.8:8080/reader/redr_verify.php&quot;,
                 data=from_data, headers=headers)
    info = session.get(&quot;http://202.196.13.8:8080/reader/redr_info.php&quot;,
                       headers=headers).content.decode(&quot;UTF-8&quot;)
    soup = BeautifulSoup(info, &quot;lxml&quot;)
    if soup.find(&#39;div&#39;, {&#39;id&#39;: &#39;mylib_content&#39;}):
        mylib_info = soup.findAll(&quot;td&quot;)
        mylib_msg = soup.findAll(&quot;a&quot;, {&#39;href&#39;: &quot;book_lst.php&quot;})
        data = {
            &#39;姓名&#39;: mylib_info[1].get_text().split(&quot;：&quot;)[1],
            &#39;证件号&#39;: mylib_info[2].get_text().split(&quot;：&quot;)[1],
            &#39;累计借书&#39;: mylib_info[12].get_text().split(&quot;：&quot;)[1],
            &#39;五天内即将过期图书&#39;: mylib_msg[1].get_text().split(&quot;[&quot;)[1].split(&quot;]&quot;)[0],
            &#39;已超期图书&#39;: mylib_msg[2].get_text().split(&quot;[&quot;)[1].split(&quot;]&quot;)[0],
            &#39;欠款金额&#39;: mylib_info[14].get_text().split(&quot;：&quot;)[1],
            &#39;工作单位&#39;: mylib_info[18].get_text().split(&quot;：&quot;)[1],
            &#39;职业/职称&#39;: mylib_info[19].get_text().split(&quot;：&quot;)[1],
            &#39;性别&#39;: mylib_info[21].get_text().split(&quot;：&quot;)[1],
            &#39;出生日期&#39;: mylib_info[26].get_text().split(&quot;：&quot;)[1],
            }
        print(data)
        library_info.insert_one(data)
        session.cookies.save()
    else:
        pass


if __name__ == &#39;__main__&#39;:
    time.sleep(2)
    for i in range(541310020101, 541310020161):
        login(str(i))
</code></pre><h4 id="通过分析数据可以得到以下图表："><a href="#通过分析数据可以得到以下图表：" class="headerlink" title="通过分析数据可以得到以下图表："></a>通过分析数据可以得到以下图表：</h4><p>在jupyter中简单的分析：</p>
<pre><code># coding: utf-8
import pymongo
import charts
from functools import reduce


client = pymongo.MongoClient(&#39;localhost&#39;, 27017)
scraping = client[&#39;scraping&#39;]
library = client[&#39;library&#39;]
IdNumbers = scraping[&#39;IdNumbers&#39;]
xk15 = library[&#39;xk15&#39;]
ys15 = library[&#39;ys15&#39;]
math_all = library[&#39;math_all&#39;]
iec = library[&#39;iec_all&#39;]

all_loc = []
for ma in iec.find():
    for d in IdNumbers.find():
        if reduce(lambda x, y:  int(str(x) + str(y)), list(str(ma[&#39;身份证号&#39;]))[0: 7]) == int(d[&#39;number&#39;]):
          all_loc.append(d[&#39;city&#39;])
#           print(d[&#39;city&#39;], ma[&#39;姓名&#39;])

locs = [reduce(lambda x, y: str(x) + str(y), list(loc)[0: 2]) for loc in all_loc]
list(set(locs))

loc_times = []
for i in list(set(locs)):
    loc_times.append(locs.count(i))
print(loc_times)

def gen_data(types):
    length = 0
    if length &lt;= len(locs):
        for area, times in zip(list(set(locs)), loc_times):
            data = {
                &#39;name&#39;: area,
                &#39;data&#39;: [times],
                &#39;type&#39;: types,
            }
            yield data
            length += 1

for i in gen_data(&#39;column&#39;):
    print(i)

series = [data for data in gen_data(&#39;column&#39;)]
charts.plot(series, show=&#39;inline&#39;, options=dict(title=dict(text=&#39;学院学生分布&#39;)))

def gen_datas():
    for area, times in zip(list(set(locs)), loc_times):
        yield [area, times]

for i in gen_datas():
    print(i)

options = {
    &#39;chart&#39;   : {&#39;zoomType&#39;:&#39;xy&#39;},
    &#39;title&#39;   : {&#39;text&#39;: &#39; 数学学院&#39;},
    &#39;subtitle&#39;: {&#39;text&#39;: &#39;学生分布&#39;},
    }
series =  [{
    &#39;type&#39;: &#39;pie&#39;,
    &#39;name&#39;: &#39;pie charts&#39;,
    &#39;data&#39;:[i for i in gen_datas()]

        }]
charts.plot(series,options=options,show=&#39;inline&#39;)
</code></pre><p><img src="http://oava7ylm5.bkt.clouddn.com/chart_math.png" alt="math"></p>
<p>其实我的目的是写一个能够知道我借阅的图书到期是给我发mail，提醒我，如果可以续借帮我续借的脚本，<br>当然我还在写(手动微笑)。。。</p>
<hr>
<p>更新线</p>
<p>关于图书馆借阅脚本，已经写的差不多了，代码有点挫:<a href="https://github.com/jianaosiding/spider_code/blob/master/renew.py" target="_blank" rel="external">RenewBook</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前段时间看完了python网络数据收集这本书，自己想练习一下。&lt;br&gt;联想到刚入学时的时候登陆学校图书馆网站，初始密码是学号，觉得很鸡肋，&lt;br&gt;觉得用模拟登陆和验证码识别，就可以爬下我们学院每个人的图书借阅记录和个人信息，顺便写一个自动借阅的脚本。&lt;br&gt;顿时很有兴趣，就
    
    </summary>
    
    
      <category term="spider" scheme="https://blog.whilte.cn/tags/spider/"/>
    
  </entry>
  
</feed>
